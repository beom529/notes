지금부터 코드를 수정해야되 일단 공식적인 지시는 다음과 같아.
Delete a peer from the membership list

Once the leader has detected that a process crashed, it will initiate a protocol to remove the member from the list – following a similar process with the one for adding a new member.

- Leader send a REQ message containing request id, current view id, operation type, and peer id of the peer to be removed. Operation type is DEL.
    
- All peers save the operation they must perform (request id, current view id, operation type, peer id) and send OK with matching request id and current view id.
    
- Once the leader receives matching OKs from all alive peers (request id and current view id match with the request), the leader will increment the view id and make the change to the list, and send to all peers a NEWVIEW message that contains the new view id and the corresponding updated list. All peers update their view id and membership list and print the new view id and membership list on the screen.
    
    As in the case of join, you can assume that only one peer leaves at the time and that finishing the protocol to update the list will not be interrupted by another event. The leader does not leave or crash. Only the leader needs to detect the crashed peer to update the list, the other processes even if they detect the failures, still need to wait for the leader to initiate the change.
    
    Communication should be implemented as in Part 1.
    
    TESTCASE 3: After all the peers are part of the membership, start crashing them one by one, wait for each view change to finish before crashing another process, In the end you should be left only with the leader.
    
    You can optimize the delete if you want, but the peers will make the changes permanent to the list, i.e. modify the view id and the list only when receiving the NEWVIEW message from the leader.
    
    OUTPUT: When a peer is deleted, the following should be printed (it’s the same as for adding). {peer_id:<ID>, view_id: <VIEW_ID>, leader: <LEADER_ID>,
    
    memb_list: [<COMMA_SEPARATED_MEMBERS>]}

Leader failure
Once the leader crashed, the peer with the lowest id becomes the new leader, all members should automatically know from the membership list who is the new leader. The new leader initiates a reconciliation phase as follows:

- The new leader sends a NEWLEADER message to all peers asking if they have any pending operations – operations that they have saved when they received a request for changes from the previous leader, but they did not apply to the list yet because the leader died before sending the NEWVIEW message. NEWLEADER message should contain request id, current view id, operation type; operation type is PENDING.
    
- The peers will respond with a message that contains that request, current view id, the opera- tion, ADD or DEL, and the id of the peer to be added or deleted – if they have any pending. If they do not have anything pending, the operation will be NOTHING. The new leader will finish the received operation (remember that we assume that there is only one event being handled and no cascading events) by restarting the add or delete protocol described in Part 1 or Part 3.

You can assume that the change was only one event, either an add or a delete. Communication is as in Part 1 and Part 3. You can optimize the protocol if you want.

TESTCASE 4: crash the current leader once he sent a request for removing a member, but before all processes received that request, specifically the peer that will be the new leader has not received this information. (Remember that the new leader will be the peer with the lowest id). Your testcase should show that the new leader finishes the operation correctly and the pend- ing peer is removed from the list and all peers correctly update the view id and the membership list.

OUTPUT: Messages corresponding to the previous test cases should be printed.

IMPORTANT: You do not need to handle more than one event, one process joins, or one leaves. You do not need to handle ”cascading events” – i.e. all joins and leaves finish before another event starts. The only exception is the testcase for the leader failure as described above.

The REQ, OK, NEWVIEW, NEWLEADER, JOIN are message types. ADD, DEL, PENDING are operation types. Do not use strings for them.

그리고 여기 코드야
package main  
  
import (  
    "bufio"  
    "encoding/json"    "flag"    "fmt"    "log"    "net"    "net/http"    _ "net/http/pprof"  
    "os"    "strings"    "sync"    "time")  
  
// Global Variables  
// https://pkg.go.dev/time#Duration  
var (  
    DEFAULT_COMMUNICATION_PORT int           = 3000  
    DEFAULT_HEARTBEAT_PORT     int           = 5000  
    START_DELAY                time.Duration = 1 * time.Second  
    JOIN_DELAY                 time.Duration = 0 * time.Second  
    HEARTBEAT_INTERVAL         time.Duration = 2 * time.Second  
    DNS_REQUEST_DELAY          time.Duration = 1500 * time.Millisecond  
)  
  
// Message Structtype MessageType int  
  
const (  
    JOIN MessageType = iota // 0  
    REQ  
    OK        // 1  
    NEWVIEW   // 2  
    NEWLEADER // 3  
)  
  
// Operation Structtype Operation int  
  
const (  
    NONE Operation = iota  
    ADD  
    DEL    PENDING)  
  
// Message (Payload) Structtype Message struct {  
    MessageType MessageType //Enum  
    Operation   Operation  
    SrcID       int //Destination Peer  
    DestID      int  
    RequestID   int  
    ViewID      int //Monotonically Increasing  
    LeaderID    int  
    NewPeerID   int  
    MemberList  []int //List includes members  
}  
  
// HB Message Structtype HeartbeatMessage struct {  
    ViewID int  
    SrcID  int  
    DestID int  
}  
  
var heartbeatTimestamps = make(map[int]time.Time)  
var heartbeatMutex sync.Mutex  
  
// Message State Management  
  
//nodeID is the Key  
/*  
ViewID1 : RequestID :{Req TF, OK TF}*/  
type RequestState struct {  
    SrcID  int  
    DestID int  
    REQ    bool  
    OK     bool  
}  
type ViewState struct {  
    Requests map[int]*RequestState  
    ViewID   int  
}  
  
var viewStates = make(map[int]*ViewState)  
  
// ADD  
func (viewstates *ViewState) ADD(requestID, srcID, destID int, req, ok bool) {  
    if viewstates.Requests == nil {  
       viewstates.Requests = make(map[int]*RequestState)  
    }  
    viewstates.Requests[requestID] = &RequestState{SrcID: srcID, DestID: destID, REQ: req, OK: ok}  
}  
  
// Detete  
func (viewstates *ViewState) REMOVE(requestID int) {  
    delete(viewstates.Requests, requestID)  
}  
  
// Search by RequestID (ViewID NOT)func (viewstates *ViewState) SEARCH(requestID int) (*RequestState, bool) {  
    request, exists := viewstates.Requests[requestID]  
    return request, exists  
}  
  
// Length  
func (viewstates *ViewState) LENGTH() int {  
    return len(viewstates.Requests)  
}  
  
// Update  
func (viewstates *ViewState) UPDATE(requestID int, req bool, ok bool) bool {  
    requestState, exists := viewstates.Requests[requestID]  
    if !exists {  
       log_message(VERBOSE_ERROR, "Viewstates Update Failed\n")  
       return false  
    }  
    requestState.REQ = req  
    requestState.OK = ok  
    log_message(VERBOSE_DEBUG, "Viewstates RequestID[%d] Update now.\n", requestID)  
    return true  
}  
  
// Peer Struct// In go struct member name has to start with capital letter  
// Structs : https://go.dev/tour/moretypes/2  
// Mutex : https://go.dev/tour/concurrency/9type Peer struct {  
    PeerID               int        // PeerID - row number    IP                   string     // IP address    TCPCommunicationPort int        // Port Number  
    UDPHeartbeatPort     int        // Port Number  
    Hostname             string     // Hostname  
    IsLeader             bool       // is Leader?  
    IsSelf               bool       // is Self?  
    ViewID               int        //Current View ID  
    LeaderID             int        //Current Leader ID  
    MemberList           []int      //Membership List  
    PendingOperations    []Message  //Pending Jobs  
    Mutex                sync.Mutex //Mutex  
    HeartbeatReceived    time.Time  //Received Time  
    HeartbeatStarted     bool       //isHeartBeatStarted  
}  
type PeerList struct {  
    //List  
    Peers []Peer  
    Mutex sync.Mutex  
}  
  
func GetPeerByID(peerList *PeerList, id int) (*Peer, error) {  
    peerList.Mutex.Lock()  
    defer peerList.Mutex.Unlock()  
  
    for _, peer := range peerList.Peers {  
       if peer.PeerID == id {  
          return &peer, nil  
       }  
    }  
    return nil, fmt.Errorf("GetPeerByID() peer not found! \n")  
}  
func GetPeerSelf(peerList *PeerList) (*Peer, error) {  
    peerList.Mutex.Lock()  
    defer peerList.Mutex.Unlock()  
  
    for _, peer := range peerList.Peers {  
       if peer.IsSelf {  
          return &peer, nil  
       }  
    }  
    return nil, fmt.Errorf("Where am I? \n")  
}  
func GetPeerLeader(peerList *PeerList) (*Peer, error) {  
    peerList.Mutex.Lock()  
    defer peerList.Mutex.Unlock()  
  
    for _, peer := range peerList.Peers {  
       if peer.IsLeader {  
          return &peer, nil  
       }  
    }  
    return nil, fmt.Errorf("Can't find the lead \n")  
}  
  
// File -> PeerList  
// File -> Line -> Peer -> Crating a Peerlist// Error Handling : https://pkg.go.dev/fmt  
// Methods https://go.dev/tour/methods/1  
// Declaration : https://go.dev/blog/declaration-syntax  
// nil : https://go101.org/article/nil.html  
func loadHosts(filename string) (*PeerList, error) {  
    log_message(VERBOSE_DEBUG, "1. [CONFIG] READING A HOSTFILE\n")  
  
    file, err := os.Open(filename)  
    if err != nil {  
       return nil, fmt.Errorf("no file detected: %v", err)  
    }  
    defer file.Close()  
  
    hosts := &PeerList{}  
    scanner := bufio.NewScanner(file)  
    rowNumber := 1  
  
    currentHostname, err := getSelfHostname()  
    if err != nil {  
       return nil, fmt.Errorf("[CONFIG] could not get current hostname: %v", err)  
    }  
  
    for scanner.Scan() {  
       line := strings.TrimSpace(scanner.Text())  
       if line == "" {  
          continue  
       }  
  
       hostname := line  
       if hostname != "" {  
          var ip string  
          var err error  
  
          for {  
             ip, err = getIP(hostname)  
             if err == nil {  
                break  
             }  
             time.Sleep(DNS_REQUEST_DELAY)  
          }  
          if err != nil {  
             return nil, fmt.Errorf("[Config] could not resolve IP for host: %s", hostname)  
          }  
  
          peer := Peer{  
             PeerID:               rowNumber,  
             IP:                   ip,  
             TCPCommunicationPort: DEFAULT_COMMUNICATION_PORT,  
             UDPHeartbeatPort:     DEFAULT_HEARTBEAT_PORT,  
             Hostname:             hostname,  
             IsLeader:             rowNumber == 1, //Only first one  
             IsSelf:               hostname == currentHostname,  
             HeartbeatStarted:     false,  
          }  
  
          // Leader = start with inclusion  
          // Peer = NEWVIEW will handle this          if peer.IsLeader && hostname == currentHostname {  
             peer.MemberList = []int{peer.PeerID}  
          } else {  
             peer.MemberList = []int{}  
          }  
  
          hosts.Peers = append(hosts.Peers, peer)  
          rowNumber++  
       } else {  
          log_message(VERBOSE_ERROR, "[Config] no hostname provided")  
          return nil, fmt.Errorf("[Config] no hostname provided")  
       }  
    }  
  
    log_message(VERBOSE_DEBUG, "[Config] Total [%d] Hosts loaded.\n", len(hosts.Peers))  
    return hosts, nil  
}  
func printHosts(peerList *PeerList) {  
    peerList.Mutex.Lock()  
    defer peerList.Mutex.Unlock()  
  
    log_message(VERBOSE_INFO, "%-6s %-12s %-8s %-8s %-15s %-10s %-10s %-6s %-6s %-18s %-18s\n",  
       "SrcID", "IP", "TCP Port", "UDP Port", "Hostname", "IsLeader", "IsSelf", "ViewID", "LeaderID", "MemberList", "PendingOperation")  
    for _, peer := range peerList.Peers {  
       log_message(VERBOSE_INFO, "%-6d %-12s %-8d %-8d %-15s %-10t %-10t %-6d %-6d %-18v %-18v\n",  
          peer.PeerID, peer.IP, peer.TCPCommunicationPort, peer.UDPHeartbeatPort, peer.Hostname, peer.IsLeader, peer.IsSelf, peer.ViewID, peer.LeaderID, peer.MemberList, peer.PendingOperations)  
    }  
}  
func getIP(hostname string) (string, error) {  
    ip_list, err := net.LookupIP(hostname)  
    if err != nil || len(ip_list) == 0 {  
       return "", err  
    }  
    return ip_list[0].String(), nil // First IP Address  
} // Hostname -> IP https://pkg.go.dev/net#LookupIP  
func getSelfHostname() (string, error) {  
    hostname, err := os.Hostname()  
    //If Hostname is nout in the OS  
    if err != nil {  
       return "", err  
    }  
    return hostname, nil  
} // Get my hostname https://pkg.go.dev/os#Hostname  
  
// Logger  
// type VerboseLevel int  
// https://blog.learngoprogramming.com/golang-variadic-funcs-how-to-patterns-369408f19085  
const (  
    VERBOSE_SUBMISSION = iota  
    VERBOSE_ERROR  
    VERBOSE_WARNING    VERBOSE_INFO    VERBOSE_DEBUG)  
  
var currentVerboseLevel = VERBOSE_DEBUG  
  
func log_message(verboseLevel int, format string, args ...interface{}) {  
    if verboseLevel <= currentVerboseLevel {  
       // stderr  
       fmt.Fprintf(os.Stderr, format, args...)  
    }  
}  
  
/*  
TCP Communication Server/Client  
Learned from : https://medium.com/@viktordev/socket-programming-in-go-write-a-simple-tcp-client-server-c9609edf3671  
TCP Server -> Connection Handling -> Message Classification -> Message Handling*/  
func startTCPServer(peer *Peer, peerList *PeerList) {  
    //1. TCP Listener  
    listener, err := net.Listen("tcp", fmt.Sprintf("%s:%d", peer.IP, peer.TCPCommunicationPort))  
    if err != nil {  
       log_message(VERBOSE_ERROR, "startTCPServer() Error starting TCP server:", err)  
       return  
    }  
    //A. When function ends, listening closed  
    defer listener.Close()  
    log_message(VERBOSE_INFO, "startTCPServer() TCP server started %s:%d\n", peer.IP, peer.TCPCommunicationPort)  
  
    //Accept -> GoRoutine(THRD) will do following jobs  
    //For is While in GO    for {  
       conn, err := listener.Accept()  
       if err != nil {  
          log_message(VERBOSE_ERROR, "startTCPServer() ERROR accepting connection", err)  
          continue  
       }  
       /*  
          *  GO Routine             Not completely a Thread but works like thread (Asynchronous)       */       go handleTCPConnection(conn, peer, peerList)  
    }  
}  
func handleTCPConnection(conn net.Conn, peer *Peer, peerList *PeerList) {  
    /*  
       DECODER (Deserialization)    */    defer conn.Close()  
  
    // Process incoming messages  
    decoder := json.NewDecoder(conn)  
    var msg Message  
    err := decoder.Decode(&msg)  
    if err != nil {  
       log_message(VERBOSE_ERROR, "Error decoding message: %v\n", err)  
       return  
    }  
    if msg.MessageType == JOIN && isProcessingJoin {  
       log_message(VERBOSE_ERROR, "handleconnection() JOIN IN PROGRESS %v\n", err)  
       conn.Close()  
    }  
    // Handle the decoded message  
    log_message(VERBOSE_DEBUG, "Received message: %+v\n", msg)  
    handleMessage(msg, peer, peerList)  
}  
func handleMessage(msg Message, peer *Peer, peerList *PeerList) {  
    /*  
       Classify and Toss the message into the each logic    */    switch msg.MessageType {  
    //LEADER  
    //PART1    case JOIN:  
       handleJoin(msg, peer, peerList)  
    case REQ:  
       handleReq(msg, peer, peerList)  
    case OK:  
       handleOk(msg, peer, peerList)  
    case NEWVIEW:  
       handleNewView(msg, peer, peerList)  
       //PART 4  
    case NEWLEADER:  
       handleNewLeader(msg, peer)  
    default:  
       log_message(VERBOSE_ERROR, "(TCP_SERVER) handleMessage() UNKNOWN TYPE: [%v]\n", msg.MessageType)  
    }  
}  
  
func sendNewView(peer *Peer, peerList *PeerList, newViewDestMembers []int) Message {  
  
    //dest = for  
    newViewMsg := Message{  
       MessageType: NEWVIEW,  
       SrcID:       peer.PeerID,  
       ViewID:      peer.ViewID,  
       LeaderID:    peer.PeerID,  
       MemberList:  peer.MemberList,  
    }  
  
    // send NEWVIEW to every designated ids  
    for _, memberID := range newViewDestMembers {  
       if memberID == peer.PeerID {  
          continue  
       }  
       newViewMsg.DestID = memberID  
       if err := sendMessageToPeer(newViewMsg, memberID, peerList); err != nil {  
          log_message(VERBOSE_ERROR, "Failed to send NEWVIEW to [%d]: %+v\n", memberID, err)  
       }  
    }  
  
    log_message(VERBOSE_DEBUG, "sendNewView() Sent NEWVIEW to specified members\n")  
  
    return newViewMsg  
}  
func sendReqToMembers(selfPeer *Peer, peerList *PeerList) {  
    /*  
       1. 1 ViewID       2. Multiple RequestID by src/dest       3.    */    log_message(VERBOSE_DEBUG, "sendReqToMembers() STARTING\n")  
    viewID := selfPeer.ViewID  
    currentMembers := make([]int, len(selfPeer.MemberList))  
    copy(currentMembers, selfPeer.MemberList)  
  
    /*  
       If view ID is not there, init    */    if _, exists := viewStates[viewID]; !exists {  
       viewStates[viewID] = &ViewState{  
          Requests: make(map[int]*RequestState),  
          ViewID:   viewID,  
       }  
    }  
  
    // REQ MSG Creation  
    reqMsg := Message{  
       MessageType: REQ,  
       Operation:   ADD,  
       ViewID:      viewID,  
       SrcID:       selfPeer.PeerID,  
       LeaderID:    selfPeer.PeerID,  
       NewPeerID:   currentProcessingID,  
    }  
  
    var wg sync.WaitGroup  
    for _, memberID := range currentMembers {  
       log_message(VERBOSE_DEBUG, "CURRENT TURN TO SEND REQ : [%d]\n", memberID)  
       if memberID == selfPeer.PeerID {  
          log_message(VERBOSE_DEBUG, "SELF DETECTED. SKIPPING.\n")  
          continue  
       }  
       //Request ID Issue  
       requestID := int(time.Now().UnixNano())  
       viewStates[viewID].ADD(requestID, selfPeer.PeerID, memberID, true, false)  
  
       reqMsgCopy := reqMsg  
       reqMsgCopy.RequestID = requestID  
       reqMsgCopy.DestID = memberID  
       wg.Add(1)  
       go func(memberID int, reqMsg Message) {  
          defer wg.Done()  
          log_message(VERBOSE_DEBUG, "sendReqToMembers() SENDING REQ [%v]\n", reqMsg)  
          if err := sendMessageToPeer(reqMsg, memberID, peerList); err != nil {  
             log_message(VERBOSE_ERROR, "Failed to send REQ to [%d]: %v\n", memberID, err)  
          }  
       }(memberID, reqMsgCopy)  
    }  
    wg.Wait()  
}  
  
var isProcessingJoin = false  
var currentProcessingID = 0  
  
func handleJoin(msg Message, selfPeer *Peer, peerList *PeerList) {  
    // 1. Check Leader  
    if !selfPeer.IsLeader {  
       log_message(VERBOSE_ERROR, "handleJoin() Peer is not leader.\n")  
       os.Exit(1)  
    }  
  
    // 2.Concurrent check and block  
    if isProcessingJoin {  
       log_message(VERBOSE_ERROR, "handleJoin() Already processing another JOIN request.\n")  
       return  
    }  
    isProcessingJoin = true  
    //defer func() { isProcessingJoin = false }()  
  
    log_message(VERBOSE_DEBUG, "handleJoin() RECEIVED JOIN FROM [%d]\n", msg.SrcID)  
  
    // 3. First Message  
    // Skip REQ-OK    if len(selfPeer.MemberList) == 1 && selfPeer.MemberList[0] == selfPeer.PeerID {  
       log_message(VERBOSE_DEBUG, "handleJoin() First JOIN message, SKIPPING REQ message.\n")  
  
       selfPeer.Mutex.Lock()  
       selfPeer.ViewID++  
       selfPeer.MemberList = append(selfPeer.MemberList, msg.SrcID)  
       selfPeer.Mutex.Unlock()  
  
       // New member send newview  
       newViewMsg := sendNewView(selfPeer, peerList, []int{msg.SrcID})  
  
       //Me too  
       selfPeer.Mutex.Lock()  
       if newViewMsg.MemberList != nil {  
          selfPeer.MemberList = newViewMsg.MemberList  
       }  
       selfPeer.ViewID = newViewMsg.ViewID  
       selfPeer.LeaderID = newViewMsg.LeaderID  
       selfPeer.Mutex.Unlock()  
  
       printMembership(selfPeer, peerList)  
       //log_message(VERBOSE_DEBUG, "handleJoin() Updated self after processing NEWVIEW\n")  
       isProcessingJoin = false  
       return  
    }  
  
    // 3. Not the first JOIN  
    // REQ send    currentProcessingID = msg.SrcID  
    sendReqToMembers(selfPeer, peerList)  
}  
func handleOk(msg Message, peer *Peer, peerList *PeerList) {  
    if !peer.IsLeader {  
       log_message(VERBOSE_ERROR, "handleOk() I am not a Leader! Ignore the message.\n")  
       os.Exit(1)  
    }  
  
    log_message(VERBOSE_DEBUG, "handleOk() Received OK from %d. REQ ID: %d, VIEW ID: %d\n", msg.SrcID, msg.RequestID, msg.ViewID)  
  
    // ViewState Search (ViewID,RequestID)  
    viewState, exists := viewStates[msg.ViewID]  
    if !exists {  
       log_message(VERBOSE_WARNING, "handleOk() No matching ViewID: %d\n", msg.ViewID)  
       return  
    }  
  
    // ViewState.RequestID Search  
    requestState, reqExists := viewState.SEARCH(msg.RequestID)  
    if !reqExists {  
       log_message(VERBOSE_WARNING, "handleOk() No matching RequestID: %d for ViewID: %d\n", msg.RequestID, msg.ViewID)  
       return  
    }  
  
    // If not OK yet  
    if msg.SrcID == requestState.DestID && !requestState.OK {  
       if !viewState.UPDATE(msg.RequestID, requestState.REQ, true) {  
          log_message(VERBOSE_WARNING, "handleOk() Failed to update RequestID: %d\n", msg.RequestID)  
          return  
       }  
    }  
  
    // 모든 OK 메시지가 수신되었는지 확인  
    allOkReceived := true  
    for _, req := range viewState.Requests {  
       if !req.OK {  
          allOkReceived = false  
          break  
       }  
    }  
  
    // 모든 OK 메시지가 수신되었으면 NEWVIEW 메시지 전송  
    if allOkReceived {  
       log_message(VERBOSE_DEBUG, "handleOk() All OK messages received for RequestID: %d. Sending NEWVIEW.\n", msg.RequestID)  
  
       peer.Mutex.Lock()  
       peer.ViewID++  
       peer.MemberList = append(peer.MemberList, currentProcessingID)  
       peer.Mutex.Unlock()  
  
       sendNewView(peer, peerList, peer.MemberList)  
  
       // View 상태 삭제  
       delete(viewState.Requests, msg.RequestID)  
       if len(viewState.Requests) == 0 {  
          delete(viewStates, msg.ViewID)  
       }  
       printMembership(peer, peerList)  
       isProcessingJoin = false  
    } else {  
       log_message(VERBOSE_DEBUG, "handleOk() Waiting for more OK messages for RequestID: %d\n", msg.RequestID)  
    }  
}  
  
// PEER  
func handleReq(msg Message, peer *Peer, peerList *PeerList) {  
    /*  
       [Instruction]       RECV>       Each peer saves the operation he must perform       (request id, current view id, operation type, peer id)  
       sends back an OK message containing the       request id and the current view id.  
       [Client]       1. Saves the operation       2. Send OK Message    */    log_message(VERBOSE_DEBUG, "handleReq() REQ from [%d]\n", msg.SrcID)  
  
    if msg.Operation != ADD {  
       log_message(VERBOSE_WARNING, "handleReq() NOT SUPPORTED OPERATION: %v\n", msg.Operation)  
       return  
    }  
  
    peer.Mutex.Lock()  
    peer.PendingOperations = append(peer.PendingOperations, msg)  
    peer.Mutex.Unlock()  
  
    // 2. OK Message Creation  
    log_message(VERBOSE_DEBUG, "handleReq() REQ - OK MSG creating\n")  
  
    okMsg := Message{  
       MessageType: OK,  
       RequestID:   msg.RequestID,  
       ViewID:      msg.ViewID,  
       SrcID:       peer.PeerID,  
       DestID:      msg.SrcID,  
    }  
  
    log_message(VERBOSE_DEBUG, "handleReq() REQ - OK MSG sending\n")  
  
    if err := sendMessageToPeer(okMsg, okMsg.DestID, peerList); err != nil {  
       log_message(VERBOSE_ERROR, "handleReq() Failed to send OK to leader [%d]: %v\n", msg.LeaderID, err)  
    }  
}  
func handleNewView(msg Message, peerSelf *Peer, peerList *PeerList) {  
    /*  
  
       [Client]       1. Received NEWVIEW       2. Change the current ViewID and membership list  
       [Instruction]       When receiving the NEWVIEW message all processes update their view id       and membership list and print the new view (view id and membership list).  
    */    log_message(VERBOSE_DEBUG, "handleNewView() NEWVIEW from [%d]\n", msg.SrcID)  
  
    peerSelf.Mutex.Lock()  
    log_message(VERBOSE_DEBUG, "handleNewView() Change ViewID,MemebershipList\n")  
  
    peerSelf.ViewID = msg.ViewID  
    peerSelf.MemberList = msg.MemberList  
  
    peerSelf.Mutex.Unlock()  
  
    // Print  
    printMembership(peerSelf, peerList)  
    log_message(VERBOSE_DEBUG, "handleNewView() Done!\n")  
  
    //Heartbeat  
    if !peerSelf.HeartbeatStarted {  
       peerSelf.HeartbeatStarted = true  
       go startHeartbeatProcesses(peerSelf, peerList)  
    }  
}  
func handleNewLeader(msg Message, peer *Peer) {  
    peer.Mutex.Lock()  
    peer.LeaderID = msg.LeaderID  
    peer.Mutex.Unlock()  
  
    fmt.Printf("[NEWLEADER 처리] 새로운 리더는 피어 %d입니다.\n", msg.LeaderID)  
  
    // 펜딩된 오퍼레이션 처리 (생략)  
}  
  
// Universal  
func sendMessageToPeer(msg Message, peerID int, peerList *PeerList) error {  
    log_message(VERBOSE_DEBUG, "sendMessageToPeer() send [%+v]\n", msg)  
    //1. Find whom to connect  
    var targetPeer *Peer  
    targetPeer, _ = GetPeerByID(peerList, peerID)  
    if targetPeer == nil {  
       return fmt.Errorf("sendMessageToPeer() target peer [%d] not found", peerID)  
    }  
  
    //2.Connect  
    address := fmt.Sprintf("%s:%d", targetPeer.IP, targetPeer.TCPCommunicationPort) // Make [IP]:[PORT]  
    conn, err := net.Dial("tcp", address)                                           // Conenct  
  
    if err != nil {  
       return fmt.Errorf("sendMessageToPeer() failed to connect to [%d][%s: %v", peerID, targetPeer.IP, err)  
    }  
    defer conn.Close()  
  
    //3.Serialization  
    encoder := json.NewEncoder(conn)  
    if err := encoder.Encode(msg); err != nil {  
       return fmt.Errorf("sendMessageToPeer() failed to send message to peer %d: %v", peerID, err)  
    }  
  
    return nil  
}  
  
func printMembership(selfPeer *Peer, peerList *PeerList) {  
    selfPeer.Mutex.Lock()  
    defer selfPeer.Mutex.Unlock()  
    leaderP, _ := GetPeerLeader(peerList)  
    log_message(VERBOSE_SUBMISSION, "{peer_id:%d, view_id: %d, leader: %d, memb_list: [%s]}\n",  
       selfPeer.PeerID, selfPeer.ViewID, leaderP.PeerID, intSliceToString(selfPeer.MemberList))  
} // Print peer's membership  
func intSliceToString(ints []int) string {  
    var result []string  
    for _, num := range ints {  
       result = append(result, fmt.Sprintf("%d", num))  
    }  
    return strings.Join(result, ",")  
} // Int List to String (Comma Seperated)  
  
func clientLeader(selfPeer *Peer, peerList *PeerList) {  
    log_message(VERBOSE_INFO, "main() I am the leader! NOT JOIN and just PRINT\n")  
  
    selfPeer.Mutex.Lock()  
    selfPeer.ViewID = 1  
    selfPeer.LeaderID = selfPeer.PeerID  
    selfPeer.MemberList = []int{selfPeer.PeerID} // Add myself in the member list  
    selfPeer.Mutex.Unlock()  
  
    printMembership(selfPeer, peerList)  
  
    //Heartbeat Start  
    if !selfPeer.HeartbeatStarted {  
       selfPeer.HeartbeatStarted = true  
       go startHeartbeatProcesses(selfPeer, peerList)  
    }  
}  
func clientNonLeader(selfPeer *Peer, joinDelay time.Duration, peerList *PeerList, crashDelay time.Duration) {  
    /*  
       It's not a leader    */    leaderP, err := GetPeerLeader(peerList)  
    if err != nil {  
       log_message(VERBOSE_ERROR, "clientNonLeader() Failed to get leader: %v\n", err)  
       return  
    }  
  
    log_message(VERBOSE_INFO, "clientNonLeader() I am not the leader! JOIN to %d\n", leaderP.PeerID)  
  
    joinMsg := Message{  
       MessageType: JOIN,  
       SrcID:       selfPeer.PeerID,  
       DestID:      leaderP.PeerID,  
       LeaderID:    leaderP.PeerID,  
    }  
  
    maxRetries := 10  
    retryInterval := 2 * time.Second // Wait for the first time  
  
    time.Sleep(joinDelay)  
    for attempt := 1; attempt <= maxRetries; attempt++ {  
  
       if attempt > 1 {  
          log_message(VERBOSE_INFO, "clientNonLeader() ATTEMPT : [%d]\n", attempt)  
       }  
  
       leaderPeer, err := GetPeerLeader(peerList) // Get the leader peer  
       if err != nil {  
          log_message(VERBOSE_ERROR, "clientNonLeader() Failed to get leader: %v\n", err)  
          return  
       }  
  
       err = sendMessageToPeer(joinMsg, leaderPeer.PeerID, peerList) // Use the leader's SrcID  
       if err == nil {  
          log_message(VERBOSE_INFO, "clientNonLeader() Successfully sent JOIN message to leader.\n")  
          break  
       }  
  
       log_message(VERBOSE_ERROR, "clientNonLeader() Attempt %d: Failed to send JOIN message: %v\n", attempt, err)  
       if attempt < maxRetries {  
          time.Sleep(retryInterval)  
       } else {  
          log_message(VERBOSE_ERROR, "clientNonLeader() Exceeded maximum retry attempts. Exiting.\n")  
          os.Exit(1)  
       }  
    }  
  
    //CRASH HANDLING  
    if crashDelay > 0 {  
       time.Sleep(crashDelay)  
       log_message(VERBOSE_SUBMISSION, "{peer_id:%d, view_id: %d, leader: %d, message:\"crashing\"}\n",  
          selfPeer.PeerID, selfPeer.ViewID, selfPeer.LeaderID)  
       os.Exit(0)  
    }  
}  
  
/*  
UDP Heartbeat Server/Client  
*/  
func startHeartbeatProcesses(selfPeer *Peer, peerList *PeerList) {  
    /*  
          Individual Go-Routine for each process  
       UDPServer->HandleHeartbeat    */    log_message(VERBOSE_DEBUG, "startHeartbeatProcesses NOW.\n")  
    go startUDPHeartbeatServer(selfPeer, peerList)  
    go sendHeartbeats(selfPeer, peerList)  
    go monitorHeartbeats(selfPeer, peerList)  
}  
  
/*  
UDP SERVER  
*/  
func startUDPHeartbeatServer(selfPeer *Peer, peerList *PeerList) {  
    //Address Struct Creation  
    addr := net.UDPAddr{  
       Port: selfPeer.UDPHeartbeatPort,  
       IP:   net.ParseIP(selfPeer.IP),  
    }  
    //Open a UDP Server  
    conn, err := net.ListenUDP("udp", &addr)  
    if err != nil {  
       log_message(VERBOSE_ERROR, "Error starting UDP server: %v\n", err)  
       return  
    }  
    defer conn.Close() // Auto-clear  
    log_message(VERBOSE_INFO, "startUDPHeartbeatServer() UDP server started %s:%d\n", addr.IP, addr.Port)  
    buffer := make([]byte, 1024)  
    for {  
       n, _, err := conn.ReadFromUDP(buffer)  
       if err != nil {  
          log_message(VERBOSE_ERROR, "Error reading UDP message: %v\n", err)  
          continue  
       }  
       var hbMsg HeartbeatMessage  
       /*  
          https://pkg.go.dev/encoding/       */       err = json.Unmarshal(buffer[:n], &hbMsg)  
       if err != nil {  
          log_message(VERBOSE_ERROR, "Error unmarshalling heartbeat: %v\n", err)  
          continue  
       }  
       handleHeartbeat(hbMsg, selfPeer)  
    }  
}  
func handleHeartbeat(hbMsg HeartbeatMessage, selfPeer *Peer) {  
    heartbeatMutex.Lock()  
    defer heartbeatMutex.Unlock()  
  
    // ViewID has to be same as the current ViewID  
    // OldViewID? Ignore it.    if hbMsg.ViewID != selfPeer.ViewID {  
       log_message(VERBOSE_DEBUG, "handleHeartbeat() Ignoring heartbeat from %d with old ViewID %d\n", hbMsg.SrcID, hbMsg.ViewID)  
       return  
    }  
    //log_message(VERBOSE_INFO, "handleHeartbeat() Receiving HBMSG  %d->%d [V#:%d]\n", hbMsg.SrcID, hbMsg.DestID, hbMsg.ViewID)  
    heartbeatTimestamps[hbMsg.SrcID] = time.Now()  
    //log_message(VERBOSE_DEBUG, "handleHeartbeat() [%+v]\n", heartbeatTimestamps)  
}  
  
/*  
Failure Detector  
*/  
func sendHeartbeats(selfPeer *Peer, peerList *PeerList) {  
    /*  
       https://mingrammer.com/gobyexample/tickers/       Each peer will broadcast a HEARTBEAT message every time a timeout T expires.    */    ticker := time.NewTicker(HEARTBEAT_INTERVAL)  
    defer ticker.Stop()  
  
    for {  
       <-ticker.C  
       /*  
          Dest = During the next function       */       hbMsg := HeartbeatMessage{  
          SrcID:  selfPeer.PeerID,  
          ViewID: selfPeer.ViewID,  
       }  
       sendHeartbeatToAll(hbMsg, peerList, selfPeer)  
    }  
}  
func sendHeartbeatToAll(hbMsg HeartbeatMessage, peerList *PeerList, selfPeer *Peer) {  
    // Lock the selfPeer mutex to safely access MemberList  
    selfPeer.Mutex.Lock()  
    currentMembers := make([]int, len(selfPeer.MemberList))  
    copy(currentMembers, selfPeer.MemberList)  
    selfPeer.Mutex.Unlock()  
  
    // For each member in the current membership list, send heartbeat  
    for _, memberID := range currentMembers {  
       if memberID == hbMsg.SrcID {  
          continue  
       }  
       peer, err := GetPeerByID(peerList, memberID)  
       if err != nil {  
          log_message(VERBOSE_WARNING, "sendHeartbeatToAll() Peer not found: %d\n", memberID)  
          continue  
       }  
       hbMsg.DestID = peer.PeerID  
       go sendHeartbeat(hbMsg, peer) // pass peer as a pointer  
    }  
}  
  
func sendHeartbeat(hbMsg HeartbeatMessage, peer *Peer) {  
    /*  
       Actually Sending a UDP Packet       Marshal (Serialize)    */    conn, err := net.DialUDP("udp", nil, &net.UDPAddr{  
       IP:   net.ParseIP(peer.IP),  
       Port: peer.UDPHeartbeatPort,  
    })  
    if err != nil {  
       log_message(VERBOSE_ERROR, "Error connecting to peer %d: %v\n", peer.PeerID, err)  
       return  
    }  
    defer conn.Close()  
  
    data, err := json.Marshal(hbMsg)  
    if err != nil {  
       log_message(VERBOSE_ERROR, "Error marshalling heartbeat: %v\n", err)  
       return  
    }  
    //log_message(VERBOSE_INFO, "sendHeartbeat() Sending HBMSG  %d->%d [V#:%d]\n", hbMsg.SrcID, hbMsg.DestID, hbMsg.ViewID)  
    conn.Write(data)  
}  
  
func monitorHeartbeats(selfPeer *Peer, peerList *PeerList) {  
    ticker := time.NewTicker(1000 * time.Millisecond)  
    defer ticker.Stop()  
  
    for {  
       <-ticker.C  
       checkForFailures(selfPeer, peerList)  
    }  
}  
func checkForFailures(selfPeer *Peer, peerList *PeerList) {  
    heartbeatMutex.Lock()  
    defer heartbeatMutex.Unlock()  
    /*  
       Instead of counting actual failures, uses INTERVAL time 2    */    currentTime := time.Now()  
    //log_message(VERBOSE_DEBUG, "checkForFailures() [%s]\n", heartbeatTimestamps)  
    for peerID, lastHeartbeat := range heartbeatTimestamps {  
       //log_message(VERBOSE_DEBUG, "checkForFailures() CHECKING Failure ID:%d,TIME:%s\n", peerID, currentTime.Sub(lastHeartbeat))  
       if peerID == selfPeer.PeerID {  
          continue  
       }  
       if currentTime.Sub(lastHeartbeat) > 2*HEARTBEAT_INTERVAL {  
          log_message(VERBOSE_INFO, "checkForFailures() Failure DETECTED\n")  
          /*  
             1. Fail IF the failure is detected             2. Current time - last heartbeat time          */          handlePeerFailure(peerID, selfPeer, peerList)  
       }  
    }  
}  
func handlePeerFailure(failedPeerID int, selfPeer *Peer, peerList *PeerList) {  
    log_message(VERBOSE_INFO, "handlePeerFailure() Failure Action Activated\n")  
  
    delete(heartbeatTimestamps, failedPeerID)  
  
    //is Leader?  
    isLeaderFailed := false  
    if selfPeer.LeaderID == failedPeerID {  
       isLeaderFailed = true  
    }  
  
    /*  
       All the peers still in the membership should print          {peer_id:<ID>, view_id: <VIEW_ID>, leader: <LEADER_ID>, message:"peer <PEER_ID> unreachable"}       If the peer that crashed was the leader, the following should be printed:          {peer_id:<ID>, view_id: <VIEW_ID>, leader: <LEADER_ID>, message:"peer <PEER_ID> (leader) unreachable"}    */    if isLeaderFailed {  
       log_message(VERBOSE_SUBMISSION, "{peer_id:%d, view_id: %d, leader: %d, message:\"peer %d (leader) unreachable\"}\n",  
          selfPeer.PeerID, selfPeer.ViewID, selfPeer.LeaderID, failedPeerID)  
    } else {  
       log_message(VERBOSE_SUBMISSION, "{peer_id:%d, view_id: %d, leader: %d, message:\"peer %d unreachable\"}\n",  
          selfPeer.PeerID, selfPeer.ViewID, selfPeer.LeaderID, failedPeerID)  
    }  
  
    // If it's the leader start the DELETION process  
    if selfPeer.IsLeader {  
       log_message(VERBOSE_INFO, "handlePeerFailure() I am the leader. Delete Protocol is activated.\n")  
       initiateDeleteProtocol(failedPeerID, selfPeer, peerList)  
    }  
}  
  
// Part 3  
func initiateDeleteProtocol(failedPeerID int, selfPeer *Peer, peerList *PeerList) {  
    /*  
       • Leader send a REQ message containing request id, current view id, operation type, and peer id of the peer to be removed. Operation type is DEL.    */    selfPeer.Mutex.Lock()  
    viewID := selfPeer.ViewID  
    currentMembers := make([]int, len(selfPeer.MemberList))  
    copy(currentMembers, selfPeer.MemberList)  
    selfPeer.Mutex.Unlock()  
  
    if _, exists := viewStates[viewID]; !exists {  
       viewStates[viewID] = &ViewState{  
          Requests: make(map[int]*RequestState),  
          ViewID:   viewID,  
       }  
    }  
  
    currentProcessingID = failedPeerID  
    //currentProcessingOperation = DEL  
  
    reqMsg := Message{  
       MessageType: REQ,  
       Operation:   DEL,  
       ViewID:      viewID,  
       SrcID:       selfPeer.PeerID,  
       LeaderID:    selfPeer.PeerID,  
       NewPeerID:   failedPeerID,  
    }  
  
    var wg sync.WaitGroup  
    for _, memberID := range currentMembers {  
       if memberID == selfPeer.PeerID || memberID == failedPeerID {  
          continue  
       }  
       requestID := int(time.Now().UnixNano())  
       viewStates[viewID].ADD(requestID, selfPeer.PeerID, memberID, true, false)  
  
       reqMsgCopy := reqMsg  
       reqMsgCopy.RequestID = requestID  
       reqMsgCopy.DestID = memberID  
       wg.Add(1)  
       go func(memberID int, reqMsg Message) {  
          defer wg.Done()  
          if err := sendMessageToPeer(reqMsg, memberID, peerList); err != nil {  
             log_message(VERBOSE_ERROR, "Failed to send REQ to [%d]: %v\n", memberID, err)  
          }  
       }(memberID, reqMsgCopy)  
    }  
    wg.Wait()  
}  
  
func main() {  
    go func() {  
       log.Println(http.ListenAndServe("localhost:6060", nil))  
    }()  
  
    var hostsFile string  
    var joinDelayParameter float64  
    var crashDelayParameter float64  
    var testCase4 bool  
  
    // Required  
    flag.StringVar(&hostsFile, "h", "", "REQUIRED! hostfile filename")  
    // Not required  
    flag.Float64Var(&joinDelayParameter, "d", 1, "start delay in seconds (can be a decimal)")  
    flag.Float64Var(&crashDelayParameter, "c", 0, "delay after JOIN message before crashing (in seconds)")  
    flag.BoolVar(&testCase4, "t", false, "TESTCASE 4 flag")  
  
    flag.Parse()  
    //Decimal to Time Duration  
    joinDelay := time.Duration(joinDelayParameter * float64(time.Second))  
    crashDelay := time.Duration(crashDelayParameter * float64(time.Second))  
  
    // HostFile Load  
    if hostsFile == "" {  
       log_message(VERBOSE_ERROR, "NO HOSTSFILE PROVIDED. CHECK YOUR PARAMETER.")  
       return  
    }  
    peerList, err := loadHosts(hostsFile)  
    if err != nil {  
       log_message(VERBOSE_ERROR, "(main) HOSTFILE READER FAILED!: %v\n", err)  
       return  
    }  
    printHosts(peerList)  
  
    // Self Detection  
    var selfPeer *Peer  
    selfPeer, _ = GetPeerSelf(peerList)  
    if selfPeer == nil {  
       log_message(VERBOSE_ERROR, "(main) I can't find myself")  
       return  
    }  
  
    go startTCPServer(selfPeer, peerList) // Server Running  
  
    // JOIN MESSAGE    // Am I a leader?    if selfPeer.IsLeader {  
       clientLeader(selfPeer, peerList)  
    } else {  
       clientNonLeader(selfPeer, joinDelay, peerList, crashDelay)  
    }  
  
    select {}  
}


일단 내가 조금 시도는 했어 initiateDeleteProtocol() 를 시작은 했는데 제데로 완성을 못했어. 
handleOk()
handleReq()
handleNewView() 도 까먹지 말고 수정해줘 한글로 대답해줘